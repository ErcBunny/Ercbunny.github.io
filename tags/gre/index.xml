<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>GRE on Ryan&#39;s Page</title>
        <link>https://ercbunny.github.io/tags/gre/</link>
        <description>Recent content in GRE on Ryan&#39;s Page</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Mon, 01 May 2023 21:40:00 +0800</lastBuildDate><atom:link href="https://ercbunny.github.io/tags/gre/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Extended Kalman Filter</title>
        <link>https://ercbunny.github.io/notes/230501-ekf/</link>
        <pubDate>Mon, 01 May 2023 21:40:00 +0800</pubDate>
        
        <guid>https://ercbunny.github.io/notes/230501-ekf/</guid>
        <description>&lt;img src="https://ercbunny.github.io/notes/230501-ekf/cover.jpg" alt="Featured image of post Extended Kalman Filter" /&gt;&lt;p&gt;&lt;code&gt;WIP&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This post summarizes my understanding of the Extended Kalman Filter (EKF). I once used EKF to filter noisy barometer data in my antenna tracking system. At that time I wrote in the repository:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I don&amp;rsquo;t quite understand how a kalman filter works (just for now), so the code for this part is written by Similar_Fair. Click the link below for more information: &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/sunhaobo1996/article/details/53861752&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/sunhaobo1996/article/details/53861752&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And that was on Sep 9, 2018. Four years later, I had to use EKF in my adaptive controller project and finally had it figured out.&lt;/p&gt;
&lt;h2 id=&#34;linear-kalman-filter&#34;&gt;Linear Kalman Filter&lt;/h2&gt;
&lt;p&gt;The Kalman Filter is also called the optimal linear estimator or filter, as it aims to minimize the sum of autocovariance of variables. While the original Kalman Filter works with linear systems, the EKF extends its working domain to non-linear systems by using Taylor expansion linearization. Understanding KF is the first step towards understanding the EKF. The &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1ez4y1X7eR/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;videos&lt;/a&gt; here are very helpful.&lt;/p&gt;
&lt;h3 id=&#34;problem-formulation&#34;&gt;Problem Formulation&lt;/h3&gt;
&lt;p&gt;Consider a discrete-time linear system:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{cases}
\bm{x}&lt;em&gt;k = \bm{G} \bm{x}&lt;/em&gt;{k-1} + \bm{H} \bm{u}&lt;em&gt;{k-1} + \bm{w}&lt;/em&gt;{k-1} \
\bm{z}_k = \bm{M} \bm{x}_k + \bm{v}_k
\end{cases}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;where $\bm{x}$ is the state vector, $\bm{w}$ is the process noise that is assumed to satisfy normal distribution of $N(0, \bm{Q})$, $\bm{z}$ is the observation vector, and $\bm{v}$ is the measurement noise s.t. $N(0, \bm{R})$. The dynamics of the process noise and measurement noise are unknown, which is the reason for modeling them as stochastic variables.&lt;/p&gt;
&lt;p&gt;With these two equations, the mean value of the state vector can be estimated in two ways:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{cases}
\hat{\bm{x}}&lt;em&gt;k^- = \bm{G} \bm{x}&lt;/em&gt;{k-1} + \bm{H} \bm{u}_{k-1} \
\hat{\bm{x}}_k^+ = \bm{M}^{-1} \bm{z}_k
\end{cases}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;one by propagating system states and inputs from the last time step, and one by inferring from the measured values. In most cases $\hat{\bm{x}}_k^- \neq \hat{\bm{x}}_k^+$, which means we need to combine, or &amp;ldquo;fuse&amp;rdquo;, these two estimations into one $\hat{\bm{x}}_k$. This step is often implemented using the weighted average. The weight $\rho_k$ may be different for different time steps:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
\hat{\bm{x}}_k
&amp;amp;= (1 - \rho_k) \hat{\bm{x}}_k^- + \rho_k \hat{\bm{x}}_k^+ \
&amp;amp;= \hat{\bm{x}}_k^- + \rho_k (\hat{\bm{x}}_k^+ - \hat{\bm{x}}_k^-) \
&amp;amp;= \hat{\bm{x}}_k^- + \rho_k (\bm{M}^{-1} \bm{z}_k - \hat{\bm{x}}_k^-) \
&amp;amp;= \hat{\bm{x}}_k^- + \rho_k \bm{M}^{-1} (\bm{z}_k - \bm{M} \hat{\bm{x}}_k^-) \
&amp;amp;= \hat{\bm{x}}_k^- + \bm{K}_k (\bm{z}_k - \bm{M} \hat{\bm{x}}_k^-)
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;where $\bm{K}_k = \rho_k \bm{M}^{-1}$ is often called the &amp;ldquo;Kalman gain&amp;rdquo;. Now, how to determine the value of $\bm{K}_k$? Please remember that we are looking at stochastic variables with variances: let&amp;rsquo;s define a variable for estimation error as $\bm{e}_k = \bm{x}_k - \hat{\bm{x}}_k$, and $\bm{e}_k \sim N(0, \bm{P}_k)$. We ultimately want to find a $\bm{K}_k$ that makes $\text{trace}(\bm{P}_k)$ minimal, which means minimizing the sum of error autocovariance.&lt;/p&gt;
&lt;h3 id=&#34;error-covariance-calculation&#34;&gt;Error Covariance Calculation&lt;/h3&gt;
&lt;p&gt;By the definition of covariance, we can express $\bm{P}_k$ as:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
\bm{P}_k
&amp;amp;= \mathbb{E}[\bm{e}_k \bm{e}_k^\intercal] \
&amp;amp;= \mathbb{E}[(\bm{x}_k - \hat{\bm{x}}_k) (\bm{x}_k - \hat{\bm{x}}_k^\intercal)]
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Swap $\hat{\bm{x}}_k$ with the weighted average expression, and note $\bm{x}_k - \hat{\bm{x}}_k^-$ as $\bm{e}_k^-$, we have:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
\bm{P}_k = \mathbb{E}[
&amp;amp;(\mathbb{I}-\bm{K}_k\bm{M})\bm{e}_k^-((\mathbb{I}-\bm{K}_k\bm{M})\bm{e}_k^-)^\intercal\
&amp;amp;- (\mathbb{I}-\bm{K}_k\bm{M})\bm{e}_k^-\bm{v}_k\bm{K}_k^\intercal\
&amp;amp;- \bm{K}_k \bm{v}_k {\bm{e}_k^-}^\intercal(\mathbb{I}-\bm{K}_k\bm{M})^\intercal\
&amp;amp;+ \bm{K}_k \bm{v}_k (\bm{K}_k \bm{v}_k)^\intercal]
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Then calculate the expectation of each term:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
\bm{P}_k &amp;amp;= (\mathbb{I}-\bm{K}_k\bm{M}) \mathbb{E}[\bm{e}_k^- {\bm{e}_k^-}^\intercal] (\mathbb{I}-\bm{K}_k\bm{M})^\intercal\
&amp;amp;- 0 \
&amp;amp;- 0 \
&amp;amp;+ \bm{K}_k\bm{R}{\bm{K}_k}^\intercal
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Nice, the second and third terms are all expected to be zero since $\bm{e}$ and $\bm{v}$ are not correlated. Here we encounter $\mathbb{E}[\bm{e}_k^-{\bm{e}_k^-}^\intercal]$, let&amp;rsquo;s call it $\bm{P}_k^-$. Then we can expand $\bm{P}_k$ to an even simpler form:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\bm{P}_k = \bm{P}_k^- - \bm{K}_k\bm{M}\bm{P}_k^- - (\bm{K}_k\bm{M}\bm{P}_k^-)^\intercal + \bm{K}_k\bm{M}\bm{P}_k^-(\bm{K}_k\bm{M})^\intercal+\bm{K}_k\bm{R}\bm{K}_k^\intercal
\end{equation}
$$&lt;/p&gt;
&lt;h3 id=&#34;minimizing-the-trace&#34;&gt;Minimizing the Trace&lt;/h3&gt;
&lt;p&gt;From the last equation we have:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\text{tr}(\bm{P}_k) = \text{tr}(\bm{P}_k^-) - 2\text{tr}(\bm{K}_k\bm{M}\bm{P}_k^-) + \text{tr}(\bm{K}_k\bm{M}\bm{P}_k^-(\bm{K}_k\bm{M})^\intercal) + \text{tr}(\bm{K}_k\bm{R}\bm{K}_k^\intercal)
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;which is a function of $\bm{K}_k$ with a minima at some point. Now, the problem is reduced to solving $\frac{d\text{tr}(\bm{P}_k)}{d\bm{K}_k} = 0$. There are two equations that greatly help the solving process:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\frac{d\text{tr}(\bm{A}\bm{B})}{d\bm{A}} = \bm{B}^\intercal
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\frac{d\text{tr}(\bm{A}\bm{B}{\bm{A}}^\intercal)}{d\bm{A}} = 2\bm{A}\bm{B}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Expand $\frac{d\text{tr}(\bm{P}_k)}{d\bm{K}_k} = 0$:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
\frac{d\text{tr}(\bm{P}_k)}{d\bm{K}_k} = 0 - 2(\bm{M}\bm{P}_k^-)^\intercal + 2\bm{K}_k\bm{M}\bm{P}_k^-\bm{M}^\intercal+2\bm{K}_k\bm{R} = 0
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Solving the above equation gives the Kalman gain: $\bm{K}_k = \frac{\bm{P}_k^-\bm{M}^\intercal}{\bm{M}\bm{P}_k^-\bm{M}^\intercal+\bm{R}}$. Problem solved! Well, not really. The final step is to calculate $\bm{P}_k^-$. Since&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\bm{e}_k^-=\bm{x}&lt;em&gt;k - \hat{\bm{x}}&lt;em&gt;k^-=\bm{G}\bm{e}&lt;/em&gt;{k-1}+\bm{w}&lt;/em&gt;{k-1}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;and $\bm{P}_k^-$ can be written in the expectation form, we have:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
\bm{P}&lt;em&gt;k^-
&amp;amp;= \mathbb{E}[\bm{G}\bm{e}&lt;/em&gt;{k-1}(\bm{G}\bm{e}&lt;em&gt;{k-1})^\intercal] + \bm{G}\mathbb{E}[\bm{e}&lt;/em&gt;{k-1}\bm{w}&lt;em&gt;{k-1}^\intercal]+\mathbb{E}[\bm{e}&lt;/em&gt;{k-1}\bm{w}&lt;em&gt;{k-1}^\intercal]\bm{G}^\intercal+\mathbb{E}[\bm{w}&lt;/em&gt;{k-1}\bm{w}&lt;em&gt;{k-1}^\intercal] \
&amp;amp;=\bm{G}\bm{P}&lt;/em&gt;{k-1}\bm{G}^\intercal+\bm{Q}
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Now the derivation is complete - we&amp;rsquo;ve found the optimal gain to fuse two estimations together to make covariance minimal. The complete update steps of the Kalman Filter are:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
&amp;amp;\text{1.}\ \hat{\bm{x}}&lt;em&gt;k^- = \bm{G} \bm{x}&lt;/em&gt;{k-1} + \bm{H} \bm{u}_{k-1} \
&amp;amp;\text{2.}\ \bm{P}&lt;em&gt;k^- = \bm{G}\bm{P}&lt;/em&gt;{k-1}\bm{G}^\intercal+\bm{Q} \
&amp;amp;\text{3.}\ \bm{K}_k = \frac{\bm{P}_k^-\bm{M}^\intercal}{\bm{M}\bm{P}_k^-\bm{M}^\intercal+\bm{R}} \
&amp;amp;\text{4.}\ \hat{\bm{x}}_k=\hat{\bm{x}}_k^- + \bm{K}_k (\bm{z}_k - \bm{M} \hat{\bm{x}}_k^-) \
&amp;amp;\text{5.}&lt;br&gt;
\begin{split}
\bm{P}_k &amp;amp;= \bm{P}_k^- - \bm{K}_k\bm{M}\bm{P}_k^- - (\bm{K}_k\bm{M}\bm{P}_k^-)^\intercal + \bm{K}_k\bm{M}\bm{P}_k^-(\bm{K}_k\bm{M})^\intercal+\bm{K}_k\bm{R}\bm{K}_k^\intercal \
&amp;amp;= \bm{P}_k^- - \bm{K}_k\bm{M}\bm{P}_k^-
\end{split}
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;h2 id=&#34;extended-kalman-filter&#34;&gt;Extended Kalman Filter&lt;/h2&gt;
&lt;p&gt;EKF extends the original flavored KF to nonlinear systems by using Taylor series expansion. That is it.&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
&amp;amp;\begin{cases}
\bm{x}&lt;em&gt;k = f&lt;/em&gt;{RK4}(\bm{x}&lt;em&gt;{k-1}, \bm{u}&lt;/em&gt;{k-1}, \bm{w}&lt;em&gt;{k-1})\
\bm{z}&lt;em&gt;k = g(\bm{x}&lt;/em&gt;{k},\bm{v}&lt;em&gt;k)
\end{cases} \
\rightarrow
&amp;amp;\begin{cases}
\bm{x}&lt;em&gt;k = f&lt;/em&gt;{RK4}(\hat{\bm{x}}&lt;/em&gt;{k-1}, \bm{u}&lt;/em&gt;{k-1}, \bm{0}) + \frac{\partial f_{RK4}(\hat{\bm{x}}&lt;em&gt;{k-1},\bm{u}&lt;/em&gt;{k-1},\bm{0})}{\partial \bm{x}}(\bm{x}&lt;em&gt;{k-1}-\hat{\bm{x}}&lt;/em&gt;{k-1}) + \frac{\partial f_{RK4}(\hat{\bm{x}}&lt;em&gt;{k-1},\bm{u}&lt;/em&gt;{k-1},\bm{0})}{\partial \bm{w}}\bm{w}&lt;em&gt;{k-1}\
\bm{z}&lt;em&gt;k = g(\hat{\bm{x}}&lt;/em&gt;{k},\bm{0})+\frac{\partial g(\hat{\bm{x}}&lt;/em&gt;{k},\bm{0})}{\partial \bm{x}}(\bm{x}&lt;em&gt;{k}-\hat{\bm{x}}&lt;/em&gt;{k})+\frac{\partial g(\hat{\bm{x}}&lt;em&gt;{k},\bm{0})}{\partial \bm{v}}\bm{v}&lt;/em&gt;{k}
\end{cases} \
:=
&amp;amp;\begin{cases}
\bm{x}_k = \hat{\bm{x}}&lt;em&gt;k^- + \bm{G}&lt;em&gt;k(\bm{x}&lt;/em&gt;{k-1}-\hat{\bm{x}}&lt;/em&gt;{k-1}) + \bm{W}&lt;em&gt;k\bm{w}&lt;/em&gt;{k-1} \
\bm{z}_k = \hat{\bm{z}}&lt;em&gt;k + \bm{M}&lt;em&gt;k(\bm{x}&lt;/em&gt;{k}-\hat{\bm{x}}&lt;/em&gt;{k})+\bm{V}&lt;em&gt;k\bm{v}&lt;/em&gt;{k-1}
\end{cases}
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;The linearized version is slightly different from the original linear form, but the derivation for the algorithm is largely the same. I won&amp;rsquo;t type everything again here but only give the complete result below. Interested readers are suggested to work it out independently.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Q. Quan, Introduction to multicopter design and control. Springer, 2017.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
&amp;amp;\text{1.}\ \hat{\bm{x}}&lt;em&gt;k^- = f&lt;/em&gt;{RK4}(\hat{\bm{x}}&lt;em&gt;{k-1}, \bm{u}&lt;/em&gt;{k-1}, \bm{0}) \
&amp;amp;\text{2.}\ \bm{P}&lt;em&gt;k^- = \bm{G}&lt;/em&gt;{k-1}\bm{P}&lt;em&gt;{k-1}\bm{G}&lt;/em&gt;{k-1}^\intercal+ \bm{W}&lt;em&gt;{k-1}\bm{Q}&lt;/em&gt;{k-1}\bm{W}_{k-1}^\intercal \
&amp;amp;\text{3.}\ \bm{K}_k = \frac{\bm{P}_k^-\bm{M}_k^\intercal}{\bm{M}_k\bm{P}_k^-\bm{M}_k^\intercal+\bm{V}_k\bm{R}\bm{V}_k^\intercal} \
&amp;amp;\text{4.}\ \hat{\bm{x}}_k=\hat{\bm{x}}_k^- + \bm{K}_k (\bm{z}_k - \hat{\bm{z}}_k) \
&amp;amp;\text{5.}\ \bm{P}_k = \bm{P}_k^- - \bm{K}_k\bm{M}_k\bm{P}_k^-
\end{split}
\end{equation}
$$&lt;/p&gt;
</description>
        </item>
        <item>
        <title>さよなら〜GRE</title>
        <link>https://ercbunny.github.io/notes/211212-gre/</link>
        <pubDate>Sun, 12 Dec 2021 21:40:00 +0800</pubDate>
        
        <guid>https://ercbunny.github.io/notes/211212-gre/</guid>
        <description>&lt;img src="https://ercbunny.github.io/notes/211212-gre/cover.png" alt="Featured image of post さよなら〜GRE" /&gt;&lt;hr&gt;
&lt;h2 id=&#34;先来说说结果&#34;&gt;先来说说结果&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Oct10在广外大学城考的是V150，Q166，AW4&lt;/li&gt;
&lt;li&gt;Dec12在赛格人才培训中心考的V160，Q168，AW应该不出意外4还是可以的&lt;/li&gt;
&lt;li&gt;好像两次Q都有问题啊，听说不满分不是冲国人是吗呜呜呜，反正再您*的见哇哈哈哈哈哈（请脑补荒泷派笑声&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;旅途一波三折&#34;&gt;旅途一波三折&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;最开始决定要考GRE是在暑假时套瓷的时候破了个防，HK那边没有老师明确说愿意收我，如果不考GRE那很多地方都去不了，就……应该八月底左右开始准备来着。怎么说，当时应该是高估了自己的能力，背3000也是一直在abandon（笑），觉得在雷哥网上练了几套自适应模拟题熟悉了一下题型就可以了。就这？对，就这……&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;然后当然3000都没背下来，考前是国庆还一口气把「进击的巨人」给补完了……（悲，离谱）。考试当天，起了一个大早坐地铁，到地铁站突发恶疾然后还滚去某个昏暗的WC里解决了一下……到了大学城北还得坐公交过去，下车了还得走一大段路。然后考试中基本上大脑一片空白，1080P屏幕实在是辣眼睛，鼠标和键盘声属实难顶唔。不过偶遇好多老同学哈哈哈。嘛，这次就战略性放弃了（逃&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;回到家之后还是决定二战Nov7，毕竟这时候停止自己也咽不下这口气。但具体怎么做还是很迷茫的，就先认定是词汇量不行，打算先把机经1300生词给过了一遍，之后继续认真刷3000了。后来雷宝（yyds）出分了，告诉我主要是做题，我觉得有道理，3000也就大概过了2000词然后滚去做机经了～到Nov7的时候大概刷了200左右选择填空题（Q和AW基本有手就行，就完全没管过），打算就这么迎接二战&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;离谱的事情是深大医院当时没能在合适的时间给出核算检测结果导致当天没办法考试……还好能退钱，然后赶紧预约后面的Dec12，还是广外。当时还发了个说说狠狠批判了自己&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在要求成功率1的情况下选择已验证可行的方法会比较好&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;回执凭证这些拿到手上一定一定一定要检查信息&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;还是要提前做准备，最好关键时刻前一天要double check&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;规则就是规则，不要想着侥幸；提前补救比赌规则要有用的多&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;然后就是继续刷机经咯，最后是刷了800多个选择填空题，KPP5套左右的完整Verbal……其实在Dec8左右收到广州考试被取消的短信，当初差点就想放弃了的说。最后还好深圳还有座位而且没有放弃，要不然真的就是寄了（最后发现赛格那边真的很不戳，方便，环境好，工作人员也更nice一点儿）。说实话，如果要我那个时候去考可能还真不一定就能过，所以很难不想起下面这一段话&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;那么人呐就都不知道，自己就不可以预料。你一个人的命运啊，当然要靠自我奋斗，但是也要考虑到历史的行程。我绝对不知道，我作为一个……怎么把我选到北京去了，所以…跟我讲话，说“中央都决定啦……”，我说另请高明吧。我实在我也不是谦虚，我一个……怎么跑到……去了呢？但是呢，……“大家已经研究决定了”，所以后来我就念了两首诗，叫“苟利……”，那么所以我就……。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;Dec12考试当天：7点半起床，洗漱吃早餐，放空自己（物理意义上）。8点10分左右网约车出发往市中心去，9点不到就到了考试中心，（有电梯，各种设施齐全干净，指引齐全，好评）。考完试大概一点四十左右，在华强北DJI，Apple店里溜达一下，玩一玩公共钢琴，跑去地下茶餐厅吃个午饭，天气很好，心情当然也不错～&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;一些备考建议&#34;&gt;一些备考建议&lt;/h2&gt;
&lt;h3 id=&#34;verbal&#34;&gt;Verbal&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;词汇量部分可以先认真学一遍张巍老师的机经生词伴侣以及等价词，然后用Excel过一遍3000词（这个表格是B站上某个视频下分享的，忘记来源了，侵删）；背3000的时候不要太死心眼了，实在记不住就算了，只求有个大致印象就可以，免得影响心情。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/ErcBunny/sharedDocs/raw/main/%e5%a1%ab%e7%a9%ba%e6%9c%ba%e7%bb%8f1300%e9%a2%98%e7%94%9f%e8%af%8d%e4%bc%b4%e4%be%a3.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;填空机经1300题生词伴侣.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/ErcBunny/sharedDocs/raw/main/%e7%9c%9f%e7%bb%8fGRE%e7%ad%89%e4%bb%b7%e8%af%8d%e6%b1%87%e6%80%bb.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;真经GRE等价词汇总.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/ErcBunny/sharedDocs/raw/main/%e8%a6%81%e4%bd%a0%e5%91%bd3000%e8%af%8d%e8%a1%a8.xlsx&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;要你命3000词表.xlsx&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;对我帮助比较大的是刷题。把&lt;a class=&#34;link&#34; href=&#34;https://gre.kmf.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;考满分&lt;/a&gt;（还挺好用，比雷哥网的网页写得好，评论区也好玩）的经典填空题至少刷完90个小节，当然越多越好，但是刷完90也基本差不多了。10min一个小节，不能再多了，顺便要注意练习节奏。阅读也可以做，但我自己的阅读还可以就放养了……模考可以用来练习整体节奏，我是把base+medium和hard以及KAPLAN下面popular的给做了&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;quantitative&#34;&gt;Quantitative&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;没有多余精力，没有满分需求的同学建议裸考&lt;/li&gt;
&lt;li&gt;要满分的话还是刷点题吧，有些条件概率在英语语境下我经常会寄，其他基本是有手就行&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;aw&#34;&gt;AW&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;基本观点是，逻辑清晰为上，结构直白就行，不求perfect，但求good和properly written，关键是自圆其说&lt;/li&gt;
&lt;li&gt;30min能写得越多越好，但在此之前结构完整更重要（不要虎头蛇尾）&lt;/li&gt;
&lt;li&gt;可以参考这个油主的&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=OFa8oeXXuoA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;一些视频&lt;/a&gt;，可以获得最基本的一些概念&lt;/li&gt;
&lt;li&gt;Issue（论点+论据）：
&lt;ol&gt;
&lt;li&gt;第一段=背景（题干信息）+我的论点+描述文章结构&lt;/li&gt;
&lt;li&gt;body几段的每一段（也就是为什么）=中心论点+阐述逻辑+例子+回扣论点&lt;/li&gt;
&lt;li&gt;避免绝对说法/让步段&lt;/li&gt;
&lt;li&gt;总结=rephrase in a more sententious way&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Argument（挑逻辑漏洞）：
&lt;ol&gt;
&lt;li&gt;原文论点+声明题干要求&lt;/li&gt;
&lt;li&gt;分段阐述逻辑，可以quote原文&lt;/li&gt;
&lt;li&gt;回扣题干要求&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
